Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/05/29 16:06:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Reading income csv
Reading reverse geocoding csv
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [Zip Code#101, LAT#97, LON#99, latlon#85, Community#40, Estimated Median Income#41L]
   +- CartesianProduct (Zip Code#101 = Zip Code#39)
      :- Filter isnotnull(Zip Code#101)
      :  +- SortAggregate(key=[latlon#85], functions=[first(LAT#73, false), first(LON#74, false), first(Zip Code#79, false)])
      :     +- Sort [latlon#85 ASC NULLS FIRST], false, 0
      :        +- Exchange hashpartitioning(latlon#85, 200), ENSURE_REQUIREMENTS, [plan_id=192]
      :           +- SortAggregate(key=[latlon#85], functions=[partial_first(LAT#73, false), partial_first(LON#74, false), partial_first(Zip Code#79, false)])
      :              +- Sort [latlon#85 ASC NULLS FIRST], false, 0
      :                 +- Project [LAT#73, LON#74, ZIPcode#75 AS Zip Code#79, pythonUDF0#102 AS latlon#85]
      :                    +- BatchEvalPython [<lambda>(LAT#73, LON#74)#84], [pythonUDF0#102]
      :                       +- FileScan csv [LAT#73,LON#74,ZIPcode#75] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/osboxes/Median_Household_Income_by_Zip_Code-Lo..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string,ZIPcode:string>
      +- Filter isnotnull(Zip Code#39)
         +- Scan ExistingRDD[Zip Code#39,Community#40,Estimated Median Income#41L]


== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [Zip Code#129, LAT#125, LON#127, latlon#85, Community#51, Estimated Median Income#52L]
   +- CartesianProduct (Zip Code#129 = Zip Code#50)
      :- Filter isnotnull(Zip Code#129)
      :  +- SortAggregate(key=[latlon#85], functions=[first(LAT#73, false), first(LON#74, false), first(Zip Code#79, false)])
      :     +- Sort [latlon#85 ASC NULLS FIRST], false, 0
      :        +- Exchange hashpartitioning(latlon#85, 200), ENSURE_REQUIREMENTS, [plan_id=240]
      :           +- SortAggregate(key=[latlon#85], functions=[partial_first(LAT#73, false), partial_first(LON#74, false), partial_first(Zip Code#79, false)])
      :              +- Sort [latlon#85 ASC NULLS FIRST], false, 0
      :                 +- Project [LAT#73, LON#74, ZIPcode#75 AS Zip Code#79, pythonUDF0#130 AS latlon#85]
      :                    +- BatchEvalPython [<lambda>(LAT#73, LON#74)#84], [pythonUDF0#130]
      :                       +- FileScan csv [LAT#73,LON#74,ZIPcode#75] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/osboxes/Median_Household_Income_by_Zip_Code-Lo..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string,ZIPcode:string>
      +- Filter isnotnull(Zip Code#50)
         +- Scan ExistingRDD[Zip Code#50,Community#51,Estimated Median Income#52L]


Reading data csv
24/05/29 16:06:32 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [LAT#189, LON#190, DR_NO#163, Date Rptd#164, DATE OCC#165, TIME OCC#166, AREA #167, AREA NAME#168, Rpt Dist No#169, Part 1-2#170, Crm Cd#171, Crm Cd Desc#172, Mocodes#173, Vict Age#174, Vict Sex#175, Vict Descent#176, Premis Cd#177, Premis Desc#178, Weapon Used Cd#179, Weapon Desc#180, Status#181, Status Desc#182, Crm Cd 1#183, Crm Cd 2#184, ... 5 more fields]
   +- CartesianProduct ((LAT#189 = LAT#250) AND (LON#190 = LON#252))
      :- Filter ((isnotnull(Vict Descent#176) AND isnotnull(LAT#189)) AND isnotnull(LON#190))
      :  +- FileScan csv [DR_NO#163,Date Rptd#164,DATE OCC#165,TIME OCC#166,AREA #167,AREA NAME#168,Rpt Dist No#169,Part 1-2#170,Crm Cd#171,Crm Cd Desc#172,Mocodes#173,Vict Age#174,Vict Sex#175,Vict Descent#176,Premis Cd#177,Premis Desc#178,Weapon Used Cd#179,Weapon Desc#180,Status#181,Status Desc#182,Crm Cd 1#183,Crm Cd 2#184,Crm Cd 3#185,Crm Cd 4#186,... 4 more fields] Batched: false, DataFilters: [isnotnull(Vict Descent#176), isnotnull(LAT#189), isnotnull(LON#190)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/osboxes/crime_data/Crime_Data_from_2010_to_201..., PartitionFilters: [], PushedFilters: [IsNotNull(Vict Descent), IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA :string,AREA NAME:strin...
      +- Project [Zip Code#254, LAT#250, LON#252]
         +- CartesianProduct (Zip Code#254 = Zip Code#39)
            :- Filter (isnotnull(Zip Code#254) AND (isnotnull(LAT#250) AND isnotnull(LON#252)))
            :  +- SortAggregate(key=[latlon#85], functions=[first(LAT#73, false), first(LON#74, false), first(Zip Code#79, false)])
            :     +- Sort [latlon#85 ASC NULLS FIRST], false, 0
            :        +- Exchange hashpartitioning(latlon#85, 200), ENSURE_REQUIREMENTS, [plan_id=329]
            :           +- SortAggregate(key=[latlon#85], functions=[partial_first(LAT#73, false), partial_first(LON#74, false), partial_first(Zip Code#79, false)])
            :              +- Sort [latlon#85 ASC NULLS FIRST], false, 0
            :                 +- Project [LAT#73, LON#74, ZIPcode#75 AS Zip Code#79, pythonUDF0#255 AS latlon#85]
            :                    +- BatchEvalPython [<lambda>(LAT#73, LON#74)#84], [pythonUDF0#255]
            :                       +- FileScan csv [LAT#73,LON#74,ZIPcode#75] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/osboxes/Median_Household_Income_by_Zip_Code-Lo..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string,ZIPcode:string>
            +- Project [Zip Code#39]
               +- Filter isnotnull(Zip Code#39)
                  +- Scan ExistingRDD[Zip Code#39,Community#40,Estimated Median Income#41L]


== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [Vict Descent#176, total victims#276L, Vict Descent text#1]
   +- CartesianProduct (Vict Descent#176 = Vict Descent#0)
      :- HashAggregate(keys=[Vict Descent#176], functions=[count(1)])
      :  +- Exchange hashpartitioning(Vict Descent#176, 200), ENSURE_REQUIREMENTS, [plan_id=438]
      :     +- HashAggregate(keys=[Vict Descent#176], functions=[partial_count(1)])
      :        +- Project [Vict Descent#176]
      :           +- CartesianProduct ((LAT#189 = LAT#283) AND (LON#190 = LON#285))
      :              :- Filter ((isnotnull(Vict Descent#176) AND isnotnull(LAT#189)) AND isnotnull(LON#190))
      :              :  +- FileScan csv [Vict Descent#176,LAT#189,LON#190] Batched: false, DataFilters: [isnotnull(Vict Descent#176), isnotnull(LAT#189), isnotnull(LON#190)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/osboxes/crime_data/Crime_Data_from_2010_to_201..., PartitionFilters: [], PushedFilters: [IsNotNull(Vict Descent), IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<Vict Descent:string,LAT:string,LON:string>
      :              +- Project [LAT#283, LON#285]
      :                 +- CartesianProduct (Zip Code#287 = Zip Code#39)
      :                    :- Filter (isnotnull(Zip Code#287) AND (isnotnull(LAT#283) AND isnotnull(LON#285)))
      :                    :  +- SortAggregate(key=[latlon#85], functions=[first(LAT#73, false), first(LON#74, false), first(Zip Code#79, false)])
      :                    :     +- Sort [latlon#85 ASC NULLS FIRST], false, 0
      :                    :        +- Exchange hashpartitioning(latlon#85, 200), ENSURE_REQUIREMENTS, [plan_id=428]
      :                    :           +- SortAggregate(key=[latlon#85], functions=[partial_first(LAT#73, false), partial_first(LON#74, false), partial_first(Zip Code#79, false)])
      :                    :              +- Sort [latlon#85 ASC NULLS FIRST], false, 0
      :                    :                 +- Project [LAT#73, LON#74, ZIPcode#75 AS Zip Code#79, pythonUDF0#288 AS latlon#85]
      :                    :                    +- BatchEvalPython [<lambda>(LAT#73, LON#74)#84], [pythonUDF0#288]
      :                    :                       +- FileScan csv [LAT#73,LON#74,ZIPcode#75] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/osboxes/Median_Household_Income_by_Zip_Code-Lo..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string,ZIPcode:string>
      :                    +- Project [Zip Code#39]
      :                       +- Filter isnotnull(Zip Code#39)
      :                          +- Scan ExistingRDD[Zip Code#39,Community#40,Estimated Median Income#41L]
      +- Filter isnotnull(Vict Descent#0)
         +- Scan ExistingRDD[Vict Descent#0,Vict Descent text#1]


== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [LAT#189, LON#190, DR_NO#163, Date Rptd#164, DATE OCC#165, TIME OCC#166, AREA #167, AREA NAME#168, Rpt Dist No#169, Part 1-2#170, Crm Cd#171, Crm Cd Desc#172, Mocodes#173, Vict Age#174, Vict Sex#175, Vict Descent#176, Premis Cd#177, Premis Desc#178, Weapon Used Cd#179, Weapon Desc#180, Status#181, Status Desc#182, Crm Cd 1#183, Crm Cd 2#184, ... 5 more fields]
   +- CartesianProduct ((LAT#189 = LAT#335) AND (LON#190 = LON#337))
      :- Filter ((isnotnull(Vict Descent#176) AND isnotnull(LAT#189)) AND isnotnull(LON#190))
      :  +- FileScan csv [DR_NO#163,Date Rptd#164,DATE OCC#165,TIME OCC#166,AREA #167,AREA NAME#168,Rpt Dist No#169,Part 1-2#170,Crm Cd#171,Crm Cd Desc#172,Mocodes#173,Vict Age#174,Vict Sex#175,Vict Descent#176,Premis Cd#177,Premis Desc#178,Weapon Used Cd#179,Weapon Desc#180,Status#181,Status Desc#182,Crm Cd 1#183,Crm Cd 2#184,Crm Cd 3#185,Crm Cd 4#186,... 4 more fields] Batched: false, DataFilters: [isnotnull(Vict Descent#176), isnotnull(LAT#189), isnotnull(LON#190)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/osboxes/crime_data/Crime_Data_from_2010_to_201..., PartitionFilters: [], PushedFilters: [IsNotNull(Vict Descent), IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<DR_NO:string,Date Rptd:string,DATE OCC:string,TIME OCC:string,AREA :string,AREA NAME:strin...
      +- Project [Zip Code#339, LAT#335, LON#337]
         +- CartesianProduct (Zip Code#339 = Zip Code#50)
            :- Filter (isnotnull(Zip Code#339) AND (isnotnull(LAT#335) AND isnotnull(LON#337)))
            :  +- SortAggregate(key=[latlon#85], functions=[first(LAT#73, false), first(LON#74, false), first(Zip Code#79, false)])
            :     +- Sort [latlon#85 ASC NULLS FIRST], false, 0
            :        +- Exchange hashpartitioning(latlon#85, 200), ENSURE_REQUIREMENTS, [plan_id=506]
            :           +- SortAggregate(key=[latlon#85], functions=[partial_first(LAT#73, false), partial_first(LON#74, false), partial_first(Zip Code#79, false)])
            :              +- Sort [latlon#85 ASC NULLS FIRST], false, 0
            :                 +- Project [LAT#73, LON#74, ZIPcode#75 AS Zip Code#79, pythonUDF0#340 AS latlon#85]
            :                    +- BatchEvalPython [<lambda>(LAT#73, LON#74)#84], [pythonUDF0#340]
            :                       +- FileScan csv [LAT#73,LON#74,ZIPcode#75] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/osboxes/Median_Household_Income_by_Zip_Code-Lo..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string,ZIPcode:string>
            +- Project [Zip Code#50]
               +- Filter isnotnull(Zip Code#50)
                  +- Scan ExistingRDD[Zip Code#50,Community#51,Estimated Median Income#52L]


== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [Vict Descent#176, total victims#361L, Vict Descent text#1]
   +- CartesianProduct (Vict Descent#176 = Vict Descent#0)
      :- HashAggregate(keys=[Vict Descent#176], functions=[count(1)])
      :  +- Exchange hashpartitioning(Vict Descent#176, 200), ENSURE_REQUIREMENTS, [plan_id=615]
      :     +- HashAggregate(keys=[Vict Descent#176], functions=[partial_count(1)])
      :        +- Project [Vict Descent#176]
      :           +- CartesianProduct ((LAT#189 = LAT#368) AND (LON#190 = LON#370))
      :              :- Filter ((isnotnull(Vict Descent#176) AND isnotnull(LAT#189)) AND isnotnull(LON#190))
      :              :  +- FileScan csv [Vict Descent#176,LAT#189,LON#190] Batched: false, DataFilters: [isnotnull(Vict Descent#176), isnotnull(LAT#189), isnotnull(LON#190)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/osboxes/crime_data/Crime_Data_from_2010_to_201..., PartitionFilters: [], PushedFilters: [IsNotNull(Vict Descent), IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<Vict Descent:string,LAT:string,LON:string>
      :              +- Project [LAT#368, LON#370]
      :                 +- CartesianProduct (Zip Code#372 = Zip Code#50)
      :                    :- Filter (isnotnull(Zip Code#372) AND (isnotnull(LAT#368) AND isnotnull(LON#370)))
      :                    :  +- SortAggregate(key=[latlon#85], functions=[first(LAT#73, false), first(LON#74, false), first(Zip Code#79, false)])
      :                    :     +- Sort [latlon#85 ASC NULLS FIRST], false, 0
      :                    :        +- Exchange hashpartitioning(latlon#85, 200), ENSURE_REQUIREMENTS, [plan_id=605]
      :                    :           +- SortAggregate(key=[latlon#85], functions=[partial_first(LAT#73, false), partial_first(LON#74, false), partial_first(Zip Code#79, false)])
      :                    :              +- Sort [latlon#85 ASC NULLS FIRST], false, 0
      :                    :                 +- Project [LAT#73, LON#74, ZIPcode#75 AS Zip Code#79, pythonUDF0#373 AS latlon#85]
      :                    :                    +- BatchEvalPython [<lambda>(LAT#73, LON#74)#84], [pythonUDF0#373]
      :                    :                       +- FileScan csv [LAT#73,LON#74,ZIPcode#75] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://master:9000/user/osboxes/Median_Household_Income_by_Zip_Code-Lo..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:string,LON:string,ZIPcode:string>
      :                    +- Project [Zip Code#50]
      :                       +- Filter isnotnull(Zip Code#50)
      :                          +- Scan ExistingRDD[Zip Code#50,Community#51,Estimated Median Income#52L]
      +- Filter isnotnull(Vict Descent#0)
         +- Scan ExistingRDD[Vict Descent#0,Vict Descent text#1]


Highest 3
+------------------------------+-------------+
|victim descent                |total victims|
+------------------------------+-------------+
|White                         |5020         |
|Other                         |1816         |
|Hispanic/Latin/Mexican        |761          |
|Black                         |327          |
|Unknown                       |211          |
|Other Asian                   |185          |
|Chinese                       |6            |
|Filipino                      |4            |
|Korean                        |3            |
|American Indian/Alaskan Native|3            |
|Hawaiian                      |2            |
|Pacific Islander              |2            |
+------------------------------+-------------+

Lowest 3
+------------------------------+-------------+
|victim descent                |total victims|
+------------------------------+-------------+
|Hispanic/Latin/Mexican        |14824        |
|Black                         |10458        |
|White                         |6999         |
|Other                         |3424         |
|Other Asian                   |1071         |
|Unknown                       |682          |
|Korean                        |90           |
|Chinese                       |30           |
|Japanese                      |25           |
|Filipino                      |24           |
|American Indian/Alaskan Native|10           |
|Vietnamese                    |3            |
|Cambodian                     |2            |
|Hawaiian                      |2            |
|Pacific Islander              |2            |
|Asian Indian                  |1            |
+------------------------------+-------------+


real    1m12.155s
user    0m0.520s
sys     0m0.297s

